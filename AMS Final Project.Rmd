---
title: "Regional Well Being"
author: Akash Mittal, Maged Saeed Abdo Mostafa Kharshom, Precious Prince, Franco Reinaldo
  Bonifacini
date: "`r Sys.Date()`"
output:
  pdf_document:
    fig_caption: true
    number_sections: true
---

\newpage
\tableofcontents
\newpage

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(openxlsx) # to read and write Excel files (.xlsx) in R
library(tidyr) # data format conversion, tidying and transforming data
library(tidyverse) # Whole package of libraries (dplyr, tidyr, ggplot2, readr, purrr, tibble, stringr, forcats)
library(dplyr)

library(ggplot2) # for plotting
library(reshape2) # for reshaping and transforming data
library(fpp3) # used for forecasting in R
library(GGally) # functionality of ggplot2 as it provides additional geoms and functions for data visualization
library(ggcorrplot) # used for the heatmap correlation matrix
library(corrplot) # used for the heatmap correlation matrix
library(fmsb) # used for creating radar charts
library(plotly) # used for the interactive plot
library(randomcoloR) # used for generating colors
library(RColorBrewer) # used for generating colors
library(pheatmap) # for heatmap
library(mclust)
library(cluster)
library(readr)

library(lme4) # for linear mixed effects modeling + functions for fitting and analyzing linear mixed effects models
library(insight) # functions for model-agnostic interpretation and visualization of regression models
library(MuMIn)
library(lattice)
library(Rtsne)
```

# Abstract

|   This project aims to **estimate the effect of various factors such as health, education, and income among others, on the life satisfaction of people in different regions from different countries**. For this, we carried out a robust approach with MEMs & Robust Estimators. Also, we performed a robust clustering of various factors into different clusters of life satisfacton.



# Introduction

|   For this project, we utilized the OECD database focusing on **indicators and life-satisfaction scores across various regions within different countries**. This comprehensive dataset captures diverse dimensions of well-being, such as education, employment, health, environment, and social support, which together contribute to a region's overall quality of life. The indicators allow for meaningful comparisons between regions, highlighting disparities and trends. Although the data reflects different years depending on the country, for consistency we adopted the **latest available data**.

|   It is essential to acknowledge that while some variability exists in the timing of the data, we assumed that any changes within a year or two would be minimal and unlikely to significantly alter policies or life-satisfaction scores. This approach enables us to draw relevant conclusions about the factors influencing regional well-being. By examining key metrics such as income, safety, health, and life satisfaction, this report aims to present a clear picture of well-being across OECD regions and highlight patterns that can inform future policies and initiatives.

# Original Dataset

|   To execute this project, a dataset containing the different metrics regarding well-being was downloaded and linked to a variable named **df_wb**.

```{r, echo=FALSE}
df_wb <- read.xlsx("C:\\Users\\franc\\Documents\\GitHub\\ams_final_project\\Dataset\\OECD_Well_Being.xlsx")
```

|   The variable **df_wb** contains 447 tuples and 28 columns (25 are the attributes to analyze).
```{r, echo=FALSE}
dim(df_wb)
```

\newpage

|   The variables included are the following:

1.	**Country:** Includes the name of all the countries included in the results.
2.	**Region:** Includes the name of all the cities from the countries included in the results.
3.	**Code:** Code associated to each pair country-city.
4.	**Population.with.at.least.secondary.education.(%):** Percentage of the population completing secondary education.
5.	**Employment rate (%):** Percentage of the working-age population employed.
6.  **Unemploy-ment rate (%):** Percentage of people without jobs actively seeking work.
7.  **Household disposable income per capita (USD PPP):** Average income available per person, in USD.
8.  **Homicide rate (per 100k):** Number of homicides per 100.000 people.
9.  **Mortality rate (per 1k):** Deaths per 1,000 people annually.
10. **Life expectancy:**  Average expected lifespan (years).
11. **Air pollution (level of PM2.5, µg/m³):** Fine particulate air pollution levels.
12. **Voter turnout (%):** Share of voters participating in elections.
13. **Broadband access (% of household):** Percentage of households with internet access.
14. **Internet download speed 2021-Q4 (%):** Internet speed growth/decline in 2021-Q4.
15. **Number of rooms per person:** Average living space per person.
16. **Perceived social network support  (%):** Percentage of people with available social support.
17. **Self assessment of life satisfaction (0-10):** Subjective rating of overall happiness.
18. **Education (0-10):** Regional score for education.
19. **Jobs (0-10):** Score based on employment indicators.
20. **Income (0-10):** Score for household income.
21. **Safety (0-10):** Regional score for personal safety.
22. **Health (0-10):** Score for health indicators.
23. **Environment (0-10):** Score for environmental quality.
24. **Civic engagement (0-10):** Score for public participation.
25. **Accessibility to services (0-10):** Availability of public services.
26. **Housing (0-10):** Score for housing quality and affordability.
27. **Community (0-10):** Score for social cohesion.
28. **Life satisfaction (0-10):** Overall happiness score.

|   The value type of each attribute appeared to be character, so further data pre-processing and data manipulation will be done.

```{r, echo=FALSE, results = 'hide'}
str(df_wb)
```

# Data Manipulation and EDA

## Null values

|   Before analyzing the hypothesis and attributes, a check on the data structure was conducted to prevent potential errors in the future. This involved examining both data types and null values.

|   First we checked the **null values** to determine their significance and understand which is the best action to take regaridng this matter. After checking that there were no null values, but yet we couldn't perform some calculations on the attributes, we did a more detailed analysis to realize that there were null values which were replaced by the **character ".."**.

\newpage
```{r, echo=FALSE}
wb_missing_values <- sapply(df_wb, function(col) sum(col == "..")/nrow(df_wb)*100)
as.data.frame(wb_missing_values)
```

|   Taking this into account, we concluded that there were no significance level of null values (in the form of ".."), so for the moment we decided to keep all the information and look for further actions regarding null values.

```{r, echo=FALSE}
missing_values_country <- aggregate(. ~ Country+Region, data = df_wb, function(x) sum(x == ".."))
missing_values_country <- as.data.frame(missing_values_country)

missing_values_country <- aggregate(
  rowSums(df_wb[, 4:ncol(df_wb)] == "..") ~ Country + Region,
  data = df_wb,
  FUN = sum
)

colnames(missing_values_country) <- c("Country", "Region", "n_null")

missing_values_country <- missing_values_country[missing_values_country$n_null > 0, ]

missing_values_country <- table(missing_values_country$Country)
missing_values_country <- as.data.frame(missing_values_country)
colnames(missing_values_country) <- c("Country", "n_cities")

n_country <- table(df_wb$Country)
n_country <- as.data.frame(n_country)
colnames(n_country) <- c("Country", "n_cities")

missing_by_country <- merge(n_country, missing_values_country, by = "Country")
colnames(missing_by_country) <- c("Country", "n_cities", "n_missing")
missing_by_country$p_na <- (missing_by_country$n_missing/missing_by_country$n_cities)*100

missing_by_country
```

|   Also, we can see that, when analyzing the null values by country, there were some cases that have 100% of missing values in some attributes. This is why, we decided to drop **Costa Rica, Iceland and Japan** as they had attributes without values, and this would have not been useful for our project. In addition to them, we also decided to exclude Türkiye as we found poor the information contained in the Life.Satisfaction.(0-10) attribute.

|   Moreover, for those cases with some null values, we decided to replace them with the **median of the country**. We decided this because the data is divided by city, so we could use the median of the rest cities of the country for those cities with null values. Furthermore, the median is a better option than the mean as we can avoid the influence of any possible outlier.

|   Finally, after all this data manipulation, we have the correct data type for each attribute.

```{r, echo=FALSE, results = 'hide'}
df_wb <- df_wb[!(df_wb$Country == "Costa Rica" | df_wb$Country == "Iceland" | df_wb$Country == "Japan" | df_wb$Country == "Türkiye"), ]

df_wb <- df_wb %>% 
  mutate(across(everything(), ~ ifelse(. == "..", NA, .)))

df_wb <- df_wb %>%
  mutate(across(.cols = which(!names(df_wb) %in% names(df_wb)[1:3]), 
                .fns = ~ as.numeric(.)))

df_wb <- df_wb %>% 
  group_by(Country) %>% 
  mutate(across(3:(ncol(df_wb)-1), ~ replace_na(., median(., na.rm = TRUE)))) %>% 
  ungroup()

df_wb <- as.data.frame(df_wb)

sapply(df_wb, class)
```

## Variables and Correlation
|   Continuing with the data manipulation, we decided to carry out a correlation analysis to determine the need of any further removal of attributes. For this, we decided to plot a correlation heatmap to rapidly see any pair of attributes highly correlated, and with thise, determine the removal of one of them.

```{r, echo=FALSE}
cor_matrix <- cor(df_wb[,4:ncol(df_wb)], use = "complete.obs")

cor_data <- melt(cor_matrix)

ggplot(cor_data, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, 
                       limit = c(-1, 1), space = "Lab", 
                       name = "Correlation")+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 5),
        axis.text.y = element_text(size = 5),
        axis.title.x = element_blank(),
    axis.title.y = element_blank())+
  coord_fixed()
```

| As a result of this analysis, we decided to drop the following variables, because they can be explained by others (high correlation) and probably have less information than other correlated variables (this was checked manually with the dataset):

1.  **Unemploy-ment.rate.(%):** -0.9 of correlation with Jobs.(0-10).
2.  **Life.expectancy:** 0.9 of correlation with Health.(0-10).
3.  **Internet.download.speed.2021-Q4.(%):** 0.87 of correlation with Accessiblity.to.services.(0-10).
4.  **Perceived.social.network.support.(%):** 0.83 of correlation with Community.(0-10).
5.  **Voter.turnout.(%):** 0.99 of correlation with Civic.engagement.(0-10).
6.  **Air.pollution.(level.of.PM2.5,.µg/m³):** -0.97 of correlation with Environment.(0-10).
7.  **Population.with.at.least.secondary.education.(%):** 0.99 of correlation with Education.(0-10).
8.  **Household.disposable.income.per.capita.(USD.PPP):** 0.99 of correlation with Income.(0-10).
9.  **Employment.rate.(%):** 0.92 of correlation with Jobs.(0-10).

|   Additionally, we decided to remove also Homicide.rate.(per.100k) and Mortality.rate.(per.1k) because we assumed that can be represented by Safety (0-10). Also we removed Broadband.access.(%.of.household) and Number.of.rooms.per.person as we did not consider it useful for the project. Last but not least, we removed Self.assessment.of.life.satisfaction.(0-10) as we directly used Life.satisfaction.(0-10).
```{r, echo=FALSE}
df_wb <- df_wb[, c(1:3, 18:ncol(df_wb))]
```

|   Taking into account the resulting dataset, we decided to exclude those countries that have less than 10 cities in the dataset, as we have 10 variables plus the main variable (life satisfaction).

```{r, echo=FALSE}
df_wb <- df_wb %>%
  group_by(Country) %>%                        # Agrupar por país
  filter(n_distinct(Region) > 10) %>%      # Contar ciudades únicas y filtrar
  ungroup() 
```

|   The last step of the data manipulation was to set the upper bound of the scale to 10, as there were some cases with decimals that ended up being a little bit over 10.

```{r, echo=FALSE}
df_wb[4:ncol(df_wb)] <- lapply(df_wb[4:ncol(df_wb)], function(x) ifelse(x > 10, 10, x))
```

|   So the final dataset used in the models was the following, containing 295 tuples and 14 columns (10 are the attributes to analyze, plus the life satisfaction attribute):

```{r, echo=FALSE}
dim(df_wb)

summary(df_wb)
```

\newpage

## Exploratory Data Analysis

|   After completing a rigorous data cleaning process to remove redundancies and address multicollinearity, we continued with an Exploratory Data Analysis (EDA) to obtain insights about the factors influencing life satisfaction across 15 countries.

### Life Satisfaction Distribution Across Countries

|   We began by analyzing how life satisfaction varies from one country to another. As illustrated in the box plot below, countries like the Netherlands, New Zealand, and Canada stood out with the highest average life satisfaction scores (9.36, 8.9, and 8.22, respectively). On the other hand, countries such as Greece, and Colombia reported the lowest scores.

|   Interestingly, in countries like Mexico and Colombia, we observed wide variability, suggesting significant regional differences within these nations. This led us to our **first key insight**: life satisfaction is not equally distributed across countries, and economic or social disparities likely play a role.

```{r, echo=FALSE}
#High-Level Distribution of Life Satisfaction

# Compare life satisfaction across countries

# Calculate both Average and Max Life Satisfaction for Annotation
country_avg <- df_wb %>%
  group_by(Country) %>%
  summarise(
    Average_Life_Satisfaction = mean(`Life.satisfaction.(0-10)`, na.rm = TRUE),
    Max_Life_Satisfaction = max(`Life.satisfaction.(0-10)`, na.rm = TRUE)
  )

# Boxplot with Average Annotations
ggplot(df_wb, aes(x = reorder(Country, `Life.satisfaction.(0-10)`), y = `Life.satisfaction.(0-10)`)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  geom_text(data = country_avg, 
            aes(x = Country, y = Max_Life_Satisfaction + 0.5, 
                label = round(Average_Life_Satisfaction, 2)), 
            size = 3, color = "red") +
  coord_flip() +
  ggtitle("Life Satisfaction by Country with Average Scores") +
  xlab("Country") +
  ylab("Life Satisfaction (0-10)") +
  theme_minimal()


```

### Understanding Relationships Between Metrics (Correlation Analysis)

|   Deeper in our analysis, we explored the correlations between life satisfaction and other well-being metrics using a heatmap:

1.    **Housing** had the strongest correlation with life satisfaction (0.69), showing that access to adequate
housing and living conditions significantly influences well-being.

2.    **Jobs** metric followed closely at 0.6, highlighting the importance of employment opportunities.

3.    **Environemnt** and **Community** stood out with a correlation of 0.56 both, highlighting
that a clean and sustainable environment and, connectedness and solidarity among groups in society
are also an important part of life. 

|   Surprisingly, **Income** showed a slightly lower correlation at 0.54, suggesting that while important, economic wealth is not the sole driver of life satisfaction.

|   This analysis revealed that housing conditions, employment opportunities, environment and community are more influential for life satisfaction than income alone.

```{r, echo=FALSE}
# Heatmap of correlations

# Select only numeric columns for correlation
numeric_data <- df_wb %>% select_if(is.numeric)

# Compute the correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")



# Customize the heatmap with better formatting
corrplot(cor_matrix, 
         method = "color",       # Use color gradient
         type = "upper",         # Show upper triangle
         addCoef.col = "black",  # Add correlation coefficients in black
         tl.col = "black",       # Text labels color
         tl.cex = 0.59,           # Text labels size
         number.cex = 0.59)       # Coefficient size)

```

\newpage

### A Holistic Comparison of Countries (Radar Chart)

|   The radar chart highlights the Top 3 countries (Netherlands, Canada, and New Zealand) and the Bottom 3 countries (Poland, Greece, and Colombia) based on key life attributes. Netherlands leads in Jobs, Civic Engagement, and Accessibility to Services, achieving the highest Life Satisfaction. Canada excels in Housing, Environment, and Income, demonstrating a strong economic and living standard. New Zealand stands out with the best Community scores, fostering a strong sense of belonging.

|   Among the lowest countries, Poland performs well in Education but struggles with Environment and Housing. Colombia, despite having a better Environment than expected, lags in Housing, Accessibility to Services, and especially Safety, making it the least secure. This highlights how gaps in infrastructure, safety, and education can lead to lower life satisfaction across these countries.

```{r, echo=FALSE, fig.width=8, fig.height=6, dpi=300, out.width="100%"}
# Calculate average life satisfaction and filter Top 3 & Bottom 3 countries
country_ranking <- df_wb %>%
  group_by(Country) %>%
  summarise(Average_Life_Satisfaction = mean(`Life.satisfaction.(0-10)`, na.rm = TRUE)) %>%
  arrange(desc(Average_Life_Satisfaction))

selected_countries <- c(
  head(country_ranking$Country, 3),  # Top 3 countries
  tail(country_ranking$Country, 3)   # Bottom 3 countries
)

# Aggregate radar chart data for the selected countries
radar_data <- df_wb %>%
  filter(Country %in% selected_countries) %>%
  group_by(Country) %>%
  summarise_at(vars(`Education.(0-10)`, `Jobs.(0-10)`, `Income.(0-10)`, `Safety.(0-10)`,
                    `Health.(0-10)`, `Environment.(0-10)`, `Civic.engagement.(0-10)`,
                    `Accessiblity.to.services.(0-10)`, `Housing.(0-10)`, `Community.(0-10)`, 
                    `Life.satisfaction.(0-10)`), mean, na.rm = TRUE)

# Prepare radar chart data
max_vals <- rep(10, ncol(radar_data) - 1)  # Max scaling
min_vals <- rep(0, ncol(radar_data) - 1)   # Min scaling
radar_chart_data <- rbind(max_vals, min_vals, radar_data[,-1])

# Define vibrant colors for Top and Bottom countries
country_colors <- c("red", "blue", "green", "purple", "orange", "darkred")  # Custom strong colors
names(country_colors) <- radar_data$Country



par(mar = c(2, 2, 2, 2), xpd = TRUE)

radarchart(radar_chart_data, axistype = 1,
           pcol = country_colors[radar_data$Country],
           pfcol = scales::alpha(country_colors[radar_data$Country], 0.2),
           plwd = 3, plty = 1,
           cglcol = "grey", cglty = 1, cglwd = 0.8, vlcex = 0.7,
           title = "Top 3 and Bottom 3 Countries - Radar Chart")

legend("bottomleft", legend = radar_data$Country,
       col = country_colors[radar_data$Country], lty = 1, lwd = 3, cex = 0.8, 
       title = "Countries", bg = "white", box.lwd = 0)

```

\newpage

### Visualizing Patterns Through Scatter Plots

|   Finally, we created two plots to explore the relationships between Income, Civic Engagement, and Life Satisfaction:

1.    **Plot 1: Circle Size by Life Satisfaction, Color by Country:** This plot revealed that countries with larger circles (higher life satisfaction) tend to have higher income and civic engagement levels. For instance, the Netherlands and the United States dominate the upper-right region of the plot. Conversely, smaller circles in the lower-left region highlight countries like Greece, where income and civic engagement remain low.

```{r, echo=FALSE, fig.align='center', out.width='100%', warning=FALSE, message=FALSE}
# Interactive scatter plot: Income vs Life Satisfaction

# Custom color palette for countries
custom_colors <- c(
  "Canada" = "red", "Chile" = "orange", "Colombia" = "yellow", "France" = "blue", 
  "Germany" = "green", "Greece" = "purple", "Italy" = "cyan", "Lithuania" = "chocolate3", "Mexico" = "pink", 
  "Netherlands" = "navy", "New Zealand" = "darkgreen", "Poland" = "brown", 
  "Spain" = "violet", "United Kingdom" = "coral", "United States" = "darkred"
)


# Plot 1: Circle Size by Life Satisfaction, Color by Country

plot1 <- ggplot(df_wb, 
                aes(x = `Income.(0-10)`, 
                    y = `Civic.engagement.(0-10)`, 
                    size = `Life.satisfaction.(0-10)`, 
                    color = Country,  # Map color to Country
                    text = paste("Region:", Region, 
                                 "<br>Country:", Country, 
                                 "<br>Life Satisfaction:", `Life.satisfaction.(0-10)`))) +
  geom_point(alpha = 0.7) +  # Use only circles
  ggtitle("Plot 1: Circle Size by Life Satisfaction, Color by Country") +
  xlab("Income (0-10)") +
  ylab("Civic Engagement (0-10)") +
  theme_minimal() +
  scale_size_continuous(range = c(3, 12), name = "Life Satisfaction") +  # Adjust circle sizes
  scale_color_manual(values = custom_colors) +  # Custom country colors
  guides(color = guide_legend(title = "Country"), 
         size = guide_legend(title = "Life Satisfaction")) +
  theme(
    legend.position = "bottom",       # Move legend below the plot
    legend.box = "horizontal",        # Arrange legend items horizontally
    plot.margin = margin(10, 10, 30, 10), # Add extra margin at the bottom
    legend.text = element_text(size = 5), # Adjust legend text size
    legend.title = element_text(size = 5) # Adjust legend title size
  )

# Display Both Plots
plot1
```

\newpage

2.    **Plot 2: Different Shapes by Country, Color by Life Satisfaction:** Here, each country is represented with a unique shape, and colors reflect their life satisfaction. Countries with higher satisfaction are visibly clustered in red, whereas lower-satisfaction countries are more dispersed, particularly toward the lower income range. These visualizations allowed us to see the clear relationships between income, civic engagement, and well-being, while also identifying disparities and regional patterns.

```{r, echo=FALSE, fig.align='center', out.width='100%', warning=FALSE, message=FALSE}
# Interactive scatter plot: Income vs Life Satisfaction

# Custom color palette for countries
custom_colors <- c(
  "Canada" = "red", "Chile" = "orange", "Colombia" = "yellow", "France" = "blue", 
  "Germany" = "green", "Greece" = "purple", "Italy" = "cyan", "Lithuania" = "chocolate3", "Mexico" = "pink", 
  "Netherlands" = "navy", "New Zealand" = "darkgreen", "Poland" = "brown", 
  "Spain" = "violet", "United Kingdom" = "coral", "United States" = "darkred"
)


# Plot 2: Different Shapes by Country, Color by Life Satisfaction

plot2 <- ggplot(df_wb, 
                aes(x = `Income.(0-10)`, 
                    y = `Civic.engagement.(0-10)`, 
                    color = `Life.satisfaction.(0-10)`,  # Color by Life Satisfaction
                    shape = Country,                    # Shape by Country
                    text = paste("Region:", Region, 
                                 "<br>Country:", Country, 
                                 "<br>Life Satisfaction:", `Life.satisfaction.(0-10)`))) +
  geom_point(alpha = 0.7, size = 5) +  # Larger shapes for visibility
  ggtitle("Plot 2: Different Shapes by Country, Color by Life Satisfaction") +
  xlab("Income (0-10)") +
  ylab("Civic Engagement (0-10)") +
  theme_minimal() +
  scale_shape_manual(values = 0:20) +  # Unique shapes for up to 20 countries
  scale_color_gradient(low = "blue", high = "red") +  # Gradient color for Life Satisfaction
  guides(shape = guide_legend(title = "Country"), 
         color = guide_colorbar(title = "Life Satisfaction")) +
  theme(
    legend.position = "bottom",       # Move legend below the plot
    legend.box = "horizontal",        # Arrange legend items horizontally
    plot.margin = margin(10, 10, 30, 10), # Add extra margin at the bottom
    legend.text = element_text(size = 5), # Adjust legend text size
    legend.title = element_text(size = 5) # Adjust legend title size
  )

# Display Plot
plot2
```

\newpage

# Clustering

|   Clustering is a machine learning technique used to group similar data points into clusters. In this project, we used three clustering methods to compare and analyze the results.

|   Under clustering analysis, we explored various clustering techniques to group countries based on different features, such as life satisfaction, income, and civic engagement. The clustering methods employed include **MClust, K-Means, and K-Medoids**. We use these techniques to analyze the data, create different cluster profiles, and compare the clustering results. We also use dimensionality reduction methods like **PCA and T-SNE** for visualizing the clusters, and various statistical plots to examine the distribution and characteristics of each cluster.

## MClust – Model-Based Clustering**

|   MClust is a probabilistic clustering method that models the data using a mixture of Gaussian distributions. It is based on the assumption that each cluster follows a normal distribution, but each cluster can have different means and variances.

|   For analysing the clusters, we use BIC criteria and then we plot the classification, uncertainty and density plots.

|   Classification Plot: The classification step assigns each data point (country) to a specific cluster. The MClust model automatically performs this step and outputs the cluster assignments for each country.

|   Uncertainty Plot: Uncertainty plots show the probability of a data point belonging to a specific cluster. These plots highlight how certain or uncertain the model is about assigning a particular data point to a cluster.

|   Density Plot: Density plots visualize the distribution of the data within each cluster. These plots show how the data points are distributed across the different feature values, and help us understand the characteristics of each cluster.

## KMeans – K-Means Clustering

|   K-Means is one of the most widely used clustering algorithms. It works by partitioning the data into K clusters, where each cluster is represented by its centroid (mean of the points within that cluster).

## KMedoids – K-Medoids Clustering

|   K-Medoids is a clustering algorithm similar to K-Means, but instead of using centroids, it uses actual data points as the center of clusters, making it more robust to noise and outliers.

```{r, echo=FALSE, warning=FALSE}
df_clustering <- df_wb

# Selecting only the Numeric Columns (predictor columns) to scale the data
df_mod <- df_clustering %>% select(-Country, -Region, -Code, -`Life.satisfaction.(0-10)`)

# Scaling the dataframe
df_mod <- scale(df_mod) # But gives a matrix as output

# Converting back to dataframe
df_mod <- as.data.frame(df_mod)

```

## Performing the Model Based Clustering.

|   For this model, we used **Mclust (Model Based Clustering)**, which employs **Gaussian Mixture Models (GMM)** to perform the clustering. It automatically selects the optimal number of clusters and model type based on criteria such as the **Bayesian Information Criterion (BIC)**. This approach ensures both accuracy and interpretability in clustering. The summary shows that the _**optimal number of clusters is 4**_ **(Appendix A)**. 

|   Once we got the 4 clusters, we created a Classification and Uncertainty plots for the different clusters:

### BIC and Number of Components Used for Clustering
```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}

# Reference 1 - https://mclust-org.github.io/mclust/reference/plot.Mclust.html
# Reference 2 - https://mclust-org.github.io/mclust-book/chapters/03_cluster.html#:~:text=Model-based%20clustering%20%28MBC%29%20is%20a%20probabilistic%20approach%20to,described%20by%20a%20probability%20distribution%20with%20unknown%20parameters.

# Selecting 5 clusters and choosing an Optimal Cluster out of all the possible combinations 
wb_optimal_cst <- Mclust(df_mod, G=1:5) # setting the number of optimal clusters to be chosen from 1 to 5.

# Legend position (for styling)
legend_args <- list(x = "bottomright", ncol = 5)

# Plotting BIC to assess model fit
plot(wb_optimal_cst, what = 'BIC', legendArgs = legend_args)
```

|   By evaluating the fit of models with different numbers of clusters and covariance structures, the BIC plot confirmed that the **VEV model with four components** was the most appropriate. This indicated a good balance between model complexity and interpretability. This result reassures us that the identified clusters capture meaningful differences in well-being metrics while avoiding overfitting.

### Classification of Data Points
```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
# Plotting classification of data points
plot(wb_optimal_cst, what = 'classification', fillEllipses = TRUE)
```

|   The classification plot of data points across clusters visualized the **assignment of regions to the four clusters based on their scores in the different attributes**. The plot highlighted distinct separations between clusters, emphasizing the significant variations in regional well-being. We can see some clear separation between clusters as for example in Education, Income and Safety, among others. This visualization effectively communicated how regions with similar well-being conditions are grouped together, providing a clear understanding of how specific attributes define life satisfaction levels.

### Uncertainty
```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
# Plotting the uncertainty 
plot(wb_optimal_cst, what = 'uncertainty')
```

|   Furthermore, the uncertainty plot offered an additional layer of insight by visualizing the **degree of confidence in assigning data points to their respective clusters**. Most regions exhibited low uncertainty, indicating that the GMM model reliably classified regions into distinct clusters. However, a few regions displayed higher uncertainty, suggesting they share characteristics with multiple clusters.

### Density Distribution of Data Within Clusters
```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
# Plot density distribution of the data within clusters
plot(wb_optimal_cst, what = 'density', legendArgs = legend_args)
```

|   Last but not least, the density distribution plot for data within clusters provide an in-depth look at how key well-being attributes are distributed across the identified clusters. These plots enable a better understanding of the internal variability and the distinctiveness of each cluster. For example, metrics such as **education, income, and safety exhibit unique distributions within clusters**, highlighting the degree of homogeneity or disparity among regions in each grouping.

\newpage

|   After this, we applied **Principal Component Analysis (PCA)** to _**reduce the dataset's dimensionality**_. This allowed us to plot the identified clusters in a two-dimensional space, providing a clear and intuitive representation of their separation and structure.

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
# Plotting the Four clusters using PC Components for better visualization.
# Performing PCA using prcomp
pca <- prcomp(df_mod, scale. = TRUE)

# first two principal components
pca_data <- data.frame(pca$x[, 1:2])

# Using the cluster assignments from Mclust
pca_data$Cluster <- factor(wb_optimal_cst$classification)

# Plotting the MClust clusters on PCA plot
ggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(alpha = 0.9, size = 2) +
  theme_minimal() +
  labs(title = "PCA with the 4 Clusters from MClust", x = "Principal Component 1", y = "Principal Component 2") +
  scale_color_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d"))+
  theme(plot.title = element_text(hjust = 0.5))
```


|   Furthermore, we employed **t-SNE (t-Distributed Stochastic Neighbor Embedding)** for a better visualization. This technique maps high-dimensional data into a lower-dimensional space while preserving the local structure (cluster), offering an alternative perspective on the clustering results.

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
set.seed(218)
# Performing t-SNE
tsne <- Rtsne(df_mod, dims = 2)

# t-SNE coordinates
tsne_data <- data.frame(tsne$Y)

# Cluster assignments from Mclust
tsne_data$Cluster <- factor(wb_optimal_cst$classification)

# Plotting t-SNE with MClust clusters
ggplot(tsne_data, aes(x = X1, y = X2, color = Cluster)) +
  geom_point(alpha = 0.9, size = 2) +
  theme_minimal() +
  labs(title = "t-SNE with the 4 Clusters from MClust", x = "t-SNE Dimension 1", y = "t-SNE Dimension 2") +
  scale_color_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) +
  theme(plot.title = element_text(hjust = 0.5))

```

## Analysis of Clusters

|   After obtaining the 4 clusters, we added the assigned clusters to the original dataframe. We then calculated the means for each cluster to better understand the characteristics of the data points within each group. This step helps identify patterns and differences between the clusters.

```{r, echo=FALSE, warning=FALSE}
# Adding the cluster assignments to the original dataset
df_mod$Cluster <- factor(wb_optimal_cst$classification)

# Calculating  mean of each feature for every cluster and storing in dataframe cluster_means
cluster_means <- df_mod %>%
  group_by(Cluster) %>%
  summarise(across(everything(), mean))

# Viewing the cluster profiles
print(cluster_means)

```

|   We used a bar plot to show the contribution of each feature in every cluster, with base reference to the average (mean) value of the feature. We can mainly see that for cluster 1, all the features have a positive value, more than the average value for all parameters.

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
# Convert the cluster_means to a long format for easy plotting
cluster_means_long <- cluster_means %>%
  pivot_longer(cols = -Cluster, names_to = "Feature", values_to = "Mean_Value")

# Creating the bar plot for each feature by cluster
ggplot(cluster_means_long, aes(x = Feature, y = Mean_Value, fill = Cluster)) +
  geom_bar(stat = "identity", position = "dodge", alpha=0.9,color = "black", linewidth = 0.1) +
  labs(title = "Average Feature Values by Cluster", 
       x = "Feature", 
       y = "Average Value") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +  # Rotate x-axis labels
  scale_fill_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) +  # Cluster colors
  theme_minimal() +
  theme(plot.title = element_text(hjust=0.5))

```

|   From this table we can clearly conclude that the data highlights a clear disparity in well-being across the clusters.:
*   **Cluster 1** performs relatively well in safety (0.606).
*   **Clsuter 2** shows mixed results, including negative scores in jobs (-0.362) and income (-0.331).
*   **Cluster 3** exhibits significantly lower scores across all attributes, with particularly negative values in safety (-1.52) and income (-1.14).
*   **Cluster 4** consistently shows the highest scores in most attributes, such as income (1.73) and jobs (0.682).

|   Moreover, we generated a scatter plot of **Income vs Civic Engagement** for the four clusters, highlighting the distribution of each cluster based on these two variables.

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}

# Calculating the centroids of clusters (average value of Income and Civic Engagement for each cluster)

centroids <- df_mod %>%
  group_by(Cluster) %>%
  summarise(Income_mean = mean(`Income.(0-10)`),
            Civic_mean = mean(`Civic.engagement.(0-10)`))

# Plotting the scatter plot with centroids
ggplot(df_mod, aes(x = `Income.(0-10)`, y = `Civic.engagement.(0-10)`, color = as.factor(wb_optimal_cst$classification))) +
  geom_point(alpha = 0.9, size = 2) +
  geom_point(data = centroids, aes(x = Income_mean, y = Civic_mean), 
             color = "grey1", size = 4, shape = 8, stroke = 2) +  # Centroids as large black stars
  labs(title = "Income vs Civic Engagement (Scaled) by Cluster with Centroids", 
       x = "Income", y = "Civic Engagement", 
       color = "Cluster") +
  scale_color_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust=0.5))

```


|   We also created a heatmap displaying the average values of factors in each cluster, with the data scaled to a baseline of 0. This plot is presented alongside the average plot to provide a clear comparison of the cluster characteristics.

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}

# Calculate the mean of each feature by cluster
cluster_means <- df_mod %>%
  group_by(Cluster) %>%
  summarise(across(starts_with("Education"):starts_with("Community"), mean))

# Plot heatmap
pheatmap(as.matrix(cluster_means[, -1]), 
         cluster_rows = TRUE, cluster_cols = TRUE,
         main = "Cluster Profiles Heatmap",
         scale = "row",  # Scale variables by row (features)
         display_numbers = TRUE)

```

\newpage

|   Later on, we went into more detail by plotting the **average Life Satisfaction Score** in each cluster.

```{r, echo=FALSE, warning=FALSE}

# Adding the predicted clusters to the original dataset
df_clustering$Cluster <- wb_optimal_cst$classification

# Calculating the mean Life Satisfaction score for each cluster
life_satisfaction_avg <- df_clustering %>%
  group_by(Cluster) %>%
  summarise(Average_Life_Satisfaction = mean(`Life.satisfaction.(0-10)`))

# View the results
print(life_satisfaction_avg)
```

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
# Visualizing using a bar plot of average Life Satisfaction by cluster
ggplot(life_satisfaction_avg, aes(x = as.factor(Cluster), y = Average_Life_Satisfaction, fill = as.factor(Cluster))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Average_Life_Satisfaction, 2)),  # Adding labels (rounded to 2 decimal places)
            vjust = -0.5,  # Adjust position of text above the bar
            color = "black") +  # Color of the text
  labs(title = "Average Life Satisfaction by Cluster", x = "Cluster", y = "Average Life Satisfaction (0-10)") +
  scale_fill_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) +  # Adjust colors for clusters
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(plot.title = element_text(hjust=0.5))
```

|   The **average life satisfaction scores** show that there is not a significant difference between the average values of Cluster 1 and Cluster 4, similarly with Cluster 2 and Cluster 3.

|   So finally, we created a histogram and a density plot for the four clusters to explore how **Life Satisfaction** behaves and its distribution within each cluster.

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}

# Creating histogram and highlighting the points by 4 cluster
ggplot(df_clustering, aes(x = `Life.satisfaction.(0-10)`, fill = as.factor(Cluster))) +
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +
  labs(title = "Histogram of Life Satisfaction by Cluster", 
       x = "Life Satisfaction (0-10)", 
       y = "Frequency") +
  scale_fill_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) +  # Adjust colors for clusters
  theme_minimal()

```

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}

# Create a density plot and highlight the clusters
ggplot(df_clustering, aes(x = `Life.satisfaction.(0-10)`, color = as.factor(Cluster))) +
  geom_density(size = 1.5) +  # Adjust the line thickness
  labs(title = "Density Plot of Life Satisfaction by Cluster", 
       x = "Life Satisfaction (0-10)", 
       y = "Density") +
  scale_color_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) +  # Adjust colors for clusters
  theme_minimal() +
  theme(plot.title = element_text(hjust=0.5))

```

|   Looking at both the histogram and density plots, we could see that there was not a clear separation among the four clusters.

## K-Means

|   Continuing with our clustering analysis, we decided to do a K-Means clustering to cross check and compare with the Mclust clusters.

```{r, echo=FALSE, warning=FALSE}
# Creating a new dataframe for K-Means
df_mod_k <- df_mod %>% select(-Cluster)
```

```{r, echo=FALSE, warning=FALSE}
# K-means clustering for 10 different values of k=1 to k=10.
set.seed(123)  # Seed for reproducibility
kmeans_results <- list()
for (k in 1:10) {
  kmeans_results[[k]] <- kmeans(df_mod_k, centers = k, nstart = 100)
}
```

|   When analysing the number of k clusters, we used the **Elbow Method**, and we concluded that there is a kink at k=2, but we move ahead to choose up to 4 clusters, after which the within-cluster sum of squares starts to fade off.

```{r, echo=FALSE, warning=FALSE}

# Plotting the total within-cluster sum of squares for different k values
wss <- sapply(kmeans_results, function(x) x$tot.withinss)
plot(1:10, wss, type = "b", pch = 19, xlab = "Number of Clusters", 
     ylab = "Total Within-Cluster Sum of Squares", main = "Elbow Method for K-means")

```

```{r, echo=FALSE, warning=FALSE}
# Running K-means, k=4
kmeans_final <- kmeans(df_mod_k, centers = 4, nstart = 100)

# Adding K-means cluster assignment to the dataframe
df_mod_k$Cluster_Kmeans <- as.factor(kmeans_final$cluster)

# Overview of the cluster assignments in 5 clusters
cat("Count of Cities in Each Cluster from K-Means\n")
table(df_mod_k$Cluster_Kmeans)


```


### Comparing K-Means vs MCLUSt

|   First, we did a **contingency table** to quickly compare both models:

```{r, echo=FALSE, warning=FALSE}
# Clusters from both the Models
df_mod_k$Cluster_Mclust <- factor(wb_optimal_cst$classification) # Mclust
df_mod_k$Cluster_Kmeans <- as.factor(kmeans_final$cluster) # K-Means

# Contingency table comparing Mclust and K-means cluster assignments
cluster_comparison <- table(df_mod_k$Cluster_Mclust, df_mod_k$Cluster_Kmeans)

cluster_comparison_r_c_sum <- addmargins(cluster_comparison)

# Printing the comparison
cat("\nRows: Mclust Clusters\nColumns: K-means Clusters\n")
print(cluster_comparison_r_c_sum)
```

|   Additionally, we created a scatter plot for comparing the clusters of both models.

```{r, echo=FALSE, warning=FALSE,}
# Scatter plot comparing K-means and Mclust clustering results
ggplot(df_mod_k, aes(x = Cluster_Mclust, y = Cluster_Kmeans, color = Cluster_Mclust)) +
  geom_jitter(alpha = 0.9, size=2) +
  labs(title = "Comparison of Mclust and K-means Clustering",
       x = "Mclust Clusters", y = "K-means Clusters") +
  scale_color_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust=0.5))
```

|   The contingency table and the scatter plot show that the clusters are approximately in line from both the models. So, we go ahead and check the Rand-Index.

|   The **Rand Index Comparison** is:

```{r, echo=FALSE, warning=FALSE}

adjustedRandIndex(wb_optimal_cst$classification, kmeans_final$cluster)

```

|   This also shows that the 4 clusters made by these two models are working in agreement with each other.

## K-Medoids Clustering

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
# K-medoids 
kmedoids_res <- pam(df_mod, k = 4)  # 4 clusters

# Clustering results
table(kmedoids_res$clustering)

# Plot of the clusters
ggplot(df_mod, aes(x = `Income.(0-10)`, y = `Civic.engagement.(0-10)`, color = as.factor(kmedoids_res$clustering))) +
  geom_point(alpha = 0.8) +
  labs(title = "K-medoids (Robust K-Means)", x = "Income", y = "Civic Engagement", color = "Cluster") +
  scale_color_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898")) +  
  theme_minimal() +
  theme(plot.title = element_text(hjust="0.5"))

```

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}

kmedoids_clusters <- kmedoids_res$clustering

# Scatter plot comparing K-medoids and Mclust clustering results
ggplot(df_mod_k, aes(x = Cluster_Mclust, y = kmedoids_clusters, color = as.factor(kmedoids_clusters))) +
  geom_jitter(alpha = 0.9, size=2) +
  labs(title = "Comparison of Mclust and K-medoids Clustering",
       x = "Mclust Clusters", y = "K-medoids Clusters") +
  scale_color_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust=0.5))

```

|   We see that there is a only a minor agreement between clusters made from K-Means and K-Medoids, which is due to the reason that K-Medoids is more robust, uses representative points and doesn't work on centroids of data.

|   Furthermore, we see a strong agreement between MClust and K-Means, which means data is probably in a structure suitable for both models. Also, K-Medoids is a distance based appraoch, while MClust is probabilistic approach.

|   Moreover, the cluster wise distribution shows that Cluster 4 is majorly composed of regions from USA only (with a higher Life Satisfaction Score), further cluster 3 includes Chile & Colombia.

```{r, echo=FALSE, warning=FALSE}

# Adding the Country Column to df_mod
df_mod$Country <- df_clustering$Country

# Summary table with count of the number of entries for each country in each cluster
country_cluster_distribution <- df_mod %>%
  group_by(Cluster, Country) %>%
  summarise(Count = n(), .groups = 'drop')

# Cluster-Wise Distribution of Countries

custom_palette_15 <- c("#004949","#009292", "#490092","#006ddb","#b66dff","#F6D854", "#E41A1C", "#4DAF4A", "#A65628",
                       "#f1cbcc","#999999", "#FC8D62", "#8DA0CB", "#E78AC3", "#A6D854")

ggplot(country_cluster_distribution, aes(x = Cluster, y = Count, fill = Country)) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = Count), position = position_stack(vjust = 0.5), color = "white") +  # Counts Inside the Horizontal Strips
  coord_flip() +  # Flipping Axes For horizontal bars
  labs(title = "Cluster-wise Distribution of Countries", 
       x = "Cluster", y = "Count of Cities from Each Country") +
  scale_fill_manual(values = custom_palette_15) +  # Using the custom color palette
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

## Further Analysis for Clustering

|   After all the analysis done, we decided to further analyse **Life Satisfaction**, so we manually divided it in 4 clusters. We divided the range 0-10 into 4 groups:

1.  **0-2.5**
2.  **2.5-5**
3.  **5-7.5**
4.  **7.5-10**

```{r, echo=FALSE, warning=FALSE}

# Creating 4 clusters based on Life Satisfaction scores
df_clustering$LifeSatisfactionCluster <- cut(df_clustering$`Life.satisfaction.(0-10)`, 
                                  breaks = c(0, 2.5, 5, 7.5, 10), 
                                  labels = c("0-2.5", "2.5-5", "5-7.5", "7.5-10"), 
                                  include.lowest = TRUE)

# Checking the new column
table(df_clustering$LifeSatisfactionCluster)

```

|   We compared this **manual clustering** vs **the M-Clust model** and the **K-Mean model**.

## Manual clustering vs M-Clust model

```{r, echo=FALSE, warning=FALSE}

# Creating contingency table
mclust_clusters <- wb_optimal_cst$classification  # wb_optimal_cst contains Mclust clusters
contingency_table_mclust <- table(df_clustering$LifeSatisfactionCluster, mclust_clusters)

# Viewing the table
print(contingency_table_mclust)

```

|   Additionally, we used a PCA visualization for further comparison between these two clustering methods.

```{r, echo=FALSE, warning=FALSE}
# Perform PCA on df_mod (standardized data)
pca_res <- prcomp(df_mod[, -c(11, 12)], scale. = TRUE)

# Add cluster labels to PCA results
pca_data <- data.frame(pca_res$x, 
                       LifeSatisfactionCluster = as.factor(df_clustering$LifeSatisfactionCluster), 
                       MclustCluster = as.factor(mclust_clusters))

# PCA Plot with Manual Colors for LifeSatisfactionCluster
ggplot(pca_data, aes(x = PC1, y = PC2, color = LifeSatisfactionCluster, shape = MclustCluster)) +
  geom_point(size = 2) +
  scale_color_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) + # Colors for LifeSatisfactionCluster
  scale_shape_manual(values = c(16, 17, 18, 15, 8)) +  # Different shapes for Mclust Clusters
  labs(title = "PCA Comparison of Life Satisfaction and Mclust Clusters",
       x = "Principal Component 1", 
       y = "Principal Component 2",
       color = "Life Satisfaction Clusters", 
       shape = "Mclust Clusters") +
  theme_minimal()

```


## Manual clustering vs K-Means Cluster

```{r, echo=FALSE, warning=FALSE}

# Assuming kmeans_final$cluster contains K-Means cluster assignments
contingency_table_kmeans <- table(df_clustering$LifeSatisfactionCluster, kmeans_final$cluster)

# Viewing the table
print(contingency_table_kmeans)

```

|   Additionally we did used the PCA visualization for further comparison.

```{r, echo=FALSE, warning=FALSE}
# Add K-Means cluster labels to PCA results
pca_data$KMeansCluster <- as.factor(kmeans_final$cluster)

# PCA Plot with Manual Colors and Shapes for K-Means
ggplot(pca_data, aes(x = PC1, y = PC2, color = LifeSatisfactionCluster, shape = KMeansCluster)) +
  geom_point(size = 2) +
  scale_color_manual(values = c("#d0eab8", "#eec77d", "#ce6d1d", "#ce9898", "#740d0d")) + # Colors for Life Satisfaction Clusters
  scale_shape_manual(values = c(16, 17, 18, 15, 8)) +  # Different shapes for K-Means Clusters
  labs(title = "PCA Comparison of Life Satisfaction and K-Means Clusters",
       x = "Principal Component 1", 
       y = "Principal Component 2",
       color = "Life Satisfaction Clusters", 
       shape = "K-Means Clusters") +
  theme_minimal()
```

|   The clusters divided manually **don't overlap** the clusters made by K-Means and M-Clust. The is probably because the scores are subjective, and different people measure life satisfaction scores in different perspectives. For some income would be a major factor, and for others community would be a major factor. However, statistically, we see that the **clustering using M-Clust and K-Means show a good overlap for the 4 clusters with a rand index 0.80**.


## Conclusion of Clustering Approach

|   Finally, we compared the metrics:

```{r, echo=FALSE, warning=FALSE}

# Compute Adjusted Rand Index (ARI)
ari_kmeans_pam <- adjustedRandIndex(kmeans_final$cluster, kmedoids_res$clustering)
ari_kmeans_mclust <- adjustedRandIndex(kmeans_final$cluster, wb_optimal_cst$classification)
ari_pam_mclust <- adjustedRandIndex(kmedoids_res$clustering, wb_optimal_cst$classification)

# Print the results
print(paste("ARI between K-Means and K-Medoids:", ari_kmeans_pam))
print(paste("ARI between K-Means and Mclust:", ari_kmeans_mclust))
print(paste("ARI between K-Medoids and Mclust:", ari_pam_mclust))
```

|   We see that there is only a minor agreement between clusters made from K-Means and K-Medoids, which is due to the reason that K-Medoids is more robust, uses representative points and doesn't work on centroids of data.

|   Further, we see a strong agreement between MClust and K-Means, which means data is probably in a structure suitable for both models.

|   Also, K-Medoids is a distance based appraoch, while MClust is probabilistic approach.

|   Hence, we conclude that the clusters formed by MClust and K-Means are better representation of the available data for Life Satisfaction Scores. However, for a better robust approach, more data and a detailed analysis can be performed using further clustering techniques like DBSCAN and spectral based clustering etc.


# Fitting the Regression Models

## Model 1:Linear Regression

```{r, echo=FALSE, warning=FALSE}
fit_model_1 = lm(`Life.satisfaction.(0-10)` ~ ., data = df_wb %>% select(-Country, -Region, -Code))
summary(fit_model_1)
```

|   This **regression model explains approximately 61.3%** of the variation in life satisfaction* (adjusted R-squared = 59.9%). **Significant predictors** include **Jobs (0-10), Environment (0-10), Housing (0-10), and Community (0-10)**, all positively associated with life satisfaction, as indicated by their significant p-values (p < 0.01). Among these, **Jobs (0-10)** has the strongest effect (Estimate = 0.348). Other variables such as **Education, Income, and Safety do not show significant** associations with life satisfaction at the 5% significance level. **The residual standard error of 1.542** suggests moderate variability not explained by the model.

|   Overall, the model identifies key factors that contribute to life satisfaction but suggests some room for improvement in capturing other influences.

## Model 2: Linear Regression using Pairwise Interaction effects

```{r, echo=FALSE, warning=FALSE}
fit_model_2 <- lm(`Life.satisfaction.(0-10)` ~ .^2, data = df_wb %>% select(-Country, -Region, -Code))
```

|   The second model (**Anex B**) includes interaction terms and explains **79.5% of the variation in life satisfaction (adjusted R-squared = 74.6%)**, showing a substantial improvement in explanatory power compared to the first model. Significant main effects include **Safety (0-10), Environment (0-10), Civic engagement (0-10), and Community (0-10)**, among others.

|   Several interaction terms are also significant, such as **Education x Safety, Jobs x Income, and Safety x Accessibility to Services**, indicating that the relationship between life satisfaction and these predictors depends on other variables. The residual standard error is reduced to 1.227, suggesting better fit and less unexplained variability. However, some predictors and interactions remain insignificant, suggesting that not all modeled interactions are meaningful. This model highlights the complex, interconnected nature of factors influencing life satisfaction.

## Model 3: Model with Interaction effects based on relevance of factors.

```{r, echo=FALSE, warning=FALSE}

fit_model_3 <- lm(`Life.satisfaction.(0-10)` ~ `Education.(0-10)` + `Jobs.(0-10)` +
                    `Income.(0-10)` + `Safety.(0-10)` + `Health.(0-10)` +
                    `Environment.(0-10)` + `Civic.engagement.(0-10)` +
                    `Accessiblity.to.services.(0-10)` + `Housing.(0-10)` + `Community.(0-10)` +
                    `Education.(0-10)`:`Safety.(0-10)` + `Education.(0-10)`:`Accessiblity.to.services.(0-10)` +
                    `Education.(0-10)`:`Housing.(0-10)`+
                    `Jobs.(0-10)`:`Income.(0-10)` + `Jobs.(0-10)`:`Safety.(0-10)` +
                    `Jobs.(0-10)`:`Civic.engagement.(0-10)` +
                    `Jobs.(0-10)`:`Accessiblity.to.services.(0-10)` + `Jobs.(0-10)`:`Housing.(0-10)` +
                    `Safety.(0-10)`:`Environment.(0-10)` +
                    `Safety.(0-10)`:`Civic.engagement.(0-10)` +
                    `Safety.(0-10)`:`Accessiblity.to.services.(0-10)` +
                    `Safety.(0-10)`:`Housing.(0-10)` +
                    `Accessiblity.to.services.(0-10)`:`Community.(0-10)`,
                  data=df_wb %>% select(-Country, -Region, -Code))

summary(fit_model_3)

```

|   The regression model incorporates main effects and interaction terms to explain life satisfaction, achieving an adjusted **R-squared of 0.7316. Key positive predictors include income, community, and jobs interacting with civic engagement, while significant negative effects are observed for safety, civic engagement, and environment**. Notable interactions, such as education with accessibility to services and safety with housing, reveal nuanced relationships where factors amplify or mitigate one another’s influence. The significant interactions highlight the complexity of factors contributing to life satisfaction, suggesting the need for targeted, context-sensitive policy interventions.


## Parameters from the Three Models

### Adjusted R-square
```{r, echo=FALSE, warning=FALSE}
paste("Model 1:", summary(fit_model_1)$adj.r.squared)
paste("Model 2:", summary(fit_model_2)$adj.r.squared)
paste("Model 3:", summary(fit_model_3)$adj.r.squared)
```

|   Adjusted R2R2 measures how well the model explains the variance in the dependent variable while adjusting for the number of predictors. **Model 2** performs best with the highest adjusted R2R2 of **0.7458**, showing it explains the most variance. **Model 3** slightly trails behind with **0.7316**, but it likely avoids overfitting by focusing on fewer relevant interaction terms.

### AIC
```{r, echo=FALSE, warning=FALSE}
paste("Model 1:", AIC(fit_model_1))
paste("Model 2:", AIC(fit_model_2))
paste("Model 3:", AIC(fit_model_3))
```

|   AIC penalizes model complexity and evaluates the goodness of fit. Lower values are better. **Model 3** has the lowest AIC (**965.94**), indicating it balances fit and complexity better than the others. **Model 2** has a higher AIC (**977.15**) than Model 3, suggesting it may be slightly more complex or less efficient in terms of fit.

### BIC
```{r, echo=FALSE, warning=FALSE}
paste("Model 1:", BIC(fit_model_1))
paste("Model 2:", BIC(fit_model_2))
paste("Model 3:", BIC(fit_model_3))
```

|   BIC penalizes complexity more heavily than AIC. Lower values indicate better performance. **Model 3** again has the lowest BIC (**1057.25**), outperforming the others in terms of simplicity and fit. **Model 2** has the highest BIC (**1185.34**), suggesting over-complexity compared to Model 1 and Model 3.

\newpage

### Residuals

|   Lower RSE implies better model performance. **Model 3** has the lowest RSE (**1.261**), indicating it predicts life satisfaction with less error compared to simpler models.
```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
plot(fit_model_1$residuals)
plot(fit_model_2$residuals)
plot(fit_model_3$residuals)
```

### Normality using Q-Q Plots
```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
# Check for normality of residuals
# Model 1
hist(fit_model_1$residuals, breaks = 20)
```

```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
qqnorm(fit_model_1$residuals)
qqline(fit_model_1$residuals)

# Model 2
hist(fit_model_2$residuals, breaks = 20)
qqnorm(fit_model_2$residuals)
qqline(fit_model_2$residuals)


# Model 3
hist(fit_model_2$residuals, breaks = 20)
qqnorm(fit_model_2$residuals)
qqline(fit_model_2$residuals)

```

|   Conclusion:

*   **Model 3** strikes the best balance between goodness of fit (AIC/BIC) and prediction error (RSE).
*   **Model 2** has the highest adjusted R2R2, meaning it explains more variance but at the cost of complexity (higher BIC).
*   **Model 1** is the simplest but performs worst in all metrics, making it the least suitable.

### ANOVA

|   We used ANOVA to check which model is better from Model 1 and Model 3:

```{r, echo=FALSE, warning=FALSE}

anova(fit_model_1, fit_model_3)

```

|   **The Analysis of Variance (ANOVA) test compares Model 1 and Model 3** to assess whether the inclusion of interaction effects in Model 3 significantly improves the model’s fit. **Model 3 has fewer residual degrees of freedom** (261 vs. 274) due to the inclusion of 13 additional interaction terms, which increases the number of estimated parameters. **The Residual Sum of Squares (RSS) is considerably lower in Model 3** (415.07 vs. 651.16), indicating that it explains more variability in life satisfaction.

|   **The difference in RSS between the models is 236.09**, demonstrating that the interaction terms in Model 3 significantly enhance the model's performance. **The F-statistic (11.42)** and the highly significant **p-value (p < 2.2e-16) confirm that the inclusion of these interaction effects leads to a statistically significant improvement in model fit. While Model 3 captures the relationships in the data better**, this improvement comes with **increased complexity** due to the additional interaction terms.


## Random Intercept Model

|   Since the data has hierarchical structure, we fitted the random intercept model:

```{r, echo=FALSE, warning=FALSE}

fit_mem_ri <- lmer(
  `Life.satisfaction.(0-10)` ~ `Education.(0-10)` + `Jobs.(0-10)` + `Income.(0-10)` + 
                             `Safety.(0-10)` + `Health.(0-10)` + `Environment.(0-10)` + 
                             `Civic.engagement.(0-10)` + 
                            `Accessiblity.to.services.(0-10)` + 
                             `Housing.(0-10)` + `Community.(0-10)` + 
                             (1 | Country), 
  data = df_wb %>% select(-Region, -Code)
)

# Summary of the model
summary(fit_mem_ri)

aic_model_4 <- AIC(fit_mem_ri)
bic_model_4<-  BIC(fit_mem_ri)


```

|   **The Random Intercept Model** was fitted to account for the hierarchical structure in the data, with random intercepts for each country. This model is useful when we expect that the baseline life satisfaction differs across countries, but the relationships between predictors and life satisfaction remain consistent within each country.

|   The model's random effects show that the variance in life satisfaction between countries (Intercept) is 1.760 with a standard deviation of 1.327. The residual variance (within-country variability) is 1.903 with a standard deviation of 1.380. These results suggest that there is a moderate degree of variability both between countries and within countries.

|   For the fixed effects, the significant predictors of life satisfaction include **Education** (positive relationship, estimate = 0.24477, p-value < 0.05), **Environment** (positive relationship, estimate = 0.16097, p-value < 0.05), and **Community** (positive relationship, estimate = 0.19783, p-value < 0.001). Other variables, such as **Jobs, Income, Safety, Health, and Civic engagement**, did not show significant effects at the conventional 5% level. These results suggest that education, environmental factors, and community engagement have notable impacts on life satisfaction, while income, safety, and health appear to have a more limited influence when accounting for the hierarchical structure.

*   **Significant Effects**: Education, Environment, Community, and Civic engagement.

*   **Non-significant Effects**: Jobs, Income, Safety, Health, Housing, and Accessibility to services.

*   **REML Criterion**: 1131.8. Measure of model fit for restricted maximum likelihood (REML) estimation. Lower values of the REML criterion indicate a better-fitting model.


|   **The AIC (Akaike Information Criterion)** for this model is **1082.41**, which is a measure of the model’s goodness of fit while penalizing for complexity (number of parameters). A lower AIC indicates a better-fitting model when compared to other models. This value suggests that the model balances fit and complexity reasonably well.

|   **The BIC (Bayesian Information Criterion)** is **1129.892**, which is similar to AIC but applies a stronger penalty for the number of parameters, especially as the number of observations increases. A lower BIC value would generally be preferred for selecting simpler models that fit the data adequately, but in this case, the relatively high BIC indicates that the model may be somewhat complex.

|   Based on the AIC and BIC, **Model 3** is the most efficient and well-fitting model. **Model 4** (the random intercept model) does not outperform the simpler models in terms of AIC and BIC, though it accounts for the hierarchical structure of the data.


## Fitting the Random Slope and Random Intercept Model

### Random Intercept Model with Random Slope for Education

|   We extended the random intercept model to allow for both a **random intercept** and a **random slope** for Education across countries. This model accounts for variability in life satisfaction not only at the country level (via the random intercept) but also for differences in the relationship between education and life satisfaction across countries (via the random slope for Education).

```{r, echo=FALSE, warning=FALSE}
fit_mem_ri_slope_edu <- lmer(
  `Life.satisfaction.(0-10)` ~ `Education.(0-10)` + `Jobs.(0-10)` + `Income.(0-10)` + 
                             `Safety.(0-10)` + `Health.(0-10)` + `Environment.(0-10)` + 
                             `Civic.engagement.(0-10)` + `Accessiblity.to.services.(0-10)` + 
                             `Housing.(0-10)` + `Community.(0-10)` + 
                             (1 + `Education.(0-10)` | Country),  # Random intercept and slope for Education
  data = df_wb %>% select(-Region, -Code)
)

summary(fit_mem_ri_slope_edu)

```

|   The **REML criterion** at convergence is 1048.3, indicating the fit of the model to the data. Below is a breakdown of the key findings from the model:

1. **Random Effects**:
*   **Country**: The model estimates variability in the intercept and slope across 14 countries.
*   **Intercept**: The variance of the random intercept is 3.9982, with a standard deviation of 1.9996.
*   **Slope for Education**: The variance for the random slope of Education is 0.1283, with a standard deviation of 0.3582. The negative correlation of **-0.91** between the intercept and slope for Education suggests that countries with higher intercepts (higher life satisfaction overall) tend to have a weaker positive association between Education and life satisfaction.
*   **Residual**: The variance of the residual (unexplained by the model) is 1.8176, with a standard deviation of 1.3482.

2. **Fixed Effects**:
*   **Intercept**: The estimated intercept is 2.7197, indicating the baseline life satisfaction when all predictors are at their reference levels.
*   **Education**: The coefficient for Education is 0.18364, but it is not statistically significant (t = 1.288, p > 0.05), suggesting that the overall effect of Education on life satisfaction is not significant when accounting for random variation across countries.
*   **Jobs**: The coefficient for Jobs is 0.15171, and it is statistically significant (t = 2.091), indicating a positive relationship between jobs and life satisfaction.
*   **Income**: The coefficient for Income is -0.01495, and it is not statistically significant (t = -0.117), suggesting that income has a negligible effect on life satisfaction in this model.
*   **Safety**: The coefficient for Safety is -0.11387, and it is statistically significant (t = -2.077), suggesting a negative relationship between safety and life satisfaction.
*   **Environment**: The coefficient for Environment is 0.18495, and it is statistically significant (t = 2.616), suggesting a positive impact of environmental quality on life satisfaction.
*   **Community**: The coefficient for Community is 0.18027, and it is statistically significant (t = 4.346), indicating that a sense of community is positively associated with life satisfaction.

3. **Model Diagnostics**:
*   **The scaled residuals** show that most values are within a reasonable range, with the minimum residual being -4.0770 and the maximum being 4.1388. These values suggest that the model does not exhibit extreme outliers.
*   **The correlation of fixed effects** reveals some relationships between the predictors. For instance, there is a strong negative correlation (-0.506) between the intercept and Education, suggesting that the countries with higher baseline life satisfaction tend to have a weaker association between Education and life satisfaction.

|   The random intercept and random slope model provides insights into how life satisfaction varies both within countries and across countries. The random slope for Education suggests that while Education may have a positive effect on life satisfaction, the strength of this effect varies considerably across countries. Although the coefficient for Education is not statistically significant overall, the model reveals that the relationship between Education and life satisfaction is not uniform across countries.

|   Additionally, variables like **Community, Jobs, and Environment** show significant positive effects on life satisfaction, whereas **Safety and Income** are not significantly associated with life satisfaction in this model. This model improves our understanding by accounting for country-level variations, offering a more nuanced perspective on the factors influencing life satisfaction.

|   This model can serve as an important basis for policy discussions, especially when considering the role of Education and other factors in improving life satisfaction at a national level.


### Random Intercept Model with Random Slope for Community

|   The model used here includes random intercepts and random slopes for **Community** in order to account for varying impacts of community engagement on **Life Satisfaction** across different countries. The random slope for community allows for the relationship between community and life satisfaction to differ by country, while the random intercept accounts for the baseline differences in life satisfaction across countries.

```{r, echo=FALSE, warning=FALSE}
fit_mem_ri_slope_comm <- lmer(
  `Life.satisfaction.(0-10)` ~ `Education.(0-10)` + `Jobs.(0-10)` + `Income.(0-10)` + 
                             `Safety.(0-10)` + `Health.(0-10)` + `Environment.(0-10)` + 
                             `Civic.engagement.(0-10)` + `Accessiblity.to.services.(0-10)` + 
                             `Housing.(0-10)` + `Community.(0-10)` + 
                             (1 + `Community.(0-10)` | Country),  # Random intercept and slope for Education
  data = df_wb %>% select(-Region, -Code)
)

summary(fit_mem_ri_slope_comm)

```

1. **Random Effects**:
*   **Country (Intercept)**: The variance of the intercept is **3.257**, with a standard deviation of **1.8047**, indicating substantial variability in the baseline life satisfaction across the 14 countries in the study.
*   **Country (Community Slope)**: The variance for the random slope of **Community is 0.029**, with a standard deviation of **0.1703**, indicating that the relationship between community engagement and life satisfaction varies somewhat across countries. The negative correlation of **-0.75** suggests that countries with higher baseline life satisfaction tend to have a weaker positive relationship between community engagement and life satisfaction.
*   **Residual**: The residual variance is **1.782**, with a standard deviation of **1.3351**, representing unexplained variation in life satisfaction after accounting for both fixed and random effects.

2. **Fixed Effects**:
*   **Intercept (2.05779)**: The baseline life satisfaction, when all predictors are set to zero, is **2.05779**, and this effect is statistically significant (t = 2.121).
*   **Education (0.20139)**: The coefficient for **Education is 0.20139**, and it is **statistically significant** (t = 2.005). This suggests that higher levels of education are positively associated with life satisfaction.
*   **Jobs (0.13475)**: The effect of Jobs on life satisfaction is **statistically significt** (t = 1.957), indicating that better job satisfaction contributes positively to life satisfaction.
*   **Income (-0.04688)**: The effect of Income is **not statistically significant** (t = -0.394), suggesting that income does not have a clear relationship with life satisfaction in this model.
*   **Safety (-0.06680)**: The coefficient for Safety is **not statistically significant** (t = -1.233), implying that safety concerns do not significantly impact life satisfaction in this model.
*   **Health (-0.01616)**: The effect of Health is **not statistically significant** (t = -0.209), indicating no significant relationship between health and life satisfaction.
*   **Environment (0.15195)**: The coefficient for Environment is **statistically significant** (t = 2.243), showing a positive relationship between environmental quality and life satisfaction.
*   **Civic Engagement (-0.15918)**: The coefficient for Civic Engagement is **statistically significant** (t = -2.317), suggesting that greater civic engagement is negatively associated with life satisfaction in this model.
*   **Accessibility to Services (0.10479)**: The effect of Accessibility to Services is not statistically significant (t = 1.230), indicating that access to services does not significantly affect life satisfaction.
*   **Housing (0.14577)**: The effect of Housing on life satisfaction is **marginally significant** (t = 1.727), suggesting a potential positive relationship, though further investigation may be necessary to confirm this.
*   **Community (0.20786)**: The coefficient for Community is **statistically significant** (t = 2.977), indicating that greater community engagement is positively associated with life satisfaction. This suggests that individuals who feel more connected to their communities tend to report higher life satisfaction.

3. **Correlation of Fixed Effects**: The correlation of fixed effects shows that **Education and Community** have a slight negative correlation of **-0.048**, indicating a small inverse relationship between these two factors across countries. This suggests that countries with higher education levels might have slightly less emphasis on community engagement, though this correlation is not large enough to be of significant concern.

4. **Model Diagnostics**:
*   **Scaled residuals** range from **-3.8163 to 3.6315**, with most residuals clustered around zero. The absence of extreme values indicates that the model fits the data reasonably well.
*   **The negative correlation** between the intercept and slope for **Community** (-0.75) suggests that in countries with higher baseline life satisfaction, the impact of community on life satisfaction is weaker. This could imply that community engagement plays a larger role in improving life satisfaction in countries with lower overall life satisfaction.

|   **The Random Intercept and Random Slope Model for Community** provides valuable insights into the factors influencing life satisfaction across countries. **Community engagement** is positively related to life satisfaction, with countries that have stronger community ties showing higher life satisfaction. **Education and Jobs** are also significant predictors of life satisfaction, with education showing a positive relationship and jobs contributing to greater satisfaction. **Civic engagement**, however, is negatively associated with life satisfaction, suggesting that greater civic participation may not always enhance life satisfaction in the countries in this study. The model reveals that **environmental quality** is a significant factor in life satisfaction, further emphasizing the importance of quality of life in both personal and community settings.


## R Squared

|   The **R-squared** values provide insights into the proportion of variance in life satisfaction explained by the models, both marginally (accounting for fixed effects) and conditionally (accounting for both fixed and random effects). Here are the results for the three models:

```{r, echo=FALSE, warning=FALSE}
r.squared_ri <- r.squaredGLMM(fit_mem_ri)

r.squared_ri_s_e <- r.squaredGLMM(fit_mem_ri_slope_edu)

r.squared_ri_s_c <- r.squaredGLMM(fit_mem_ri_slope_comm)

# Table for data

r_squared_table <- data.frame(
  Model = c("Random Intercept Model", 
            "Random Intercept with Random Slope for Education", 
            "Random Intercept with Random Slope for Community"),
  R_Squared_Marginal = c(r.squared_ri[1], r.squared_ri_s_e[1], r.squared_ri_s_c[1]),
  R_Squared_Conditional = c(r.squared_ri[2], r.squared_ri_s_e[2], r.squared_ri_s_c[2])
)

print(r_squared_table)


```

*   **The Random Intercept Model** explains **41.64%** of the variance in life satisfaction through fixed effects alone (marginal R-squared). After accounting for country-specific variations, the model explains **69.68%** of the total variance in life satisfaction (conditional R-squared).

*   ***The Random Intercept with Random Slope for Education** model marginally explains **40.35%** of the variance in life satisfaction. Including the random slopes for education results in a conditional R-squared of **71.39%**, suggesting that the random effects (differences between countries) account for a substantial portion of the variance, with education having a varying impact across countries.

*   **The Random Intercept with Random Slope for Community** model has the highest marginal R-squared at **42.77%**, indicating that the model's fixed effects explain slightly more variance compared to the others. The conditional R-squared value of **70.65%** suggests that the inclusion of random slopes for community engagement further explains the variability in life satisfaction between countries.

|   In summary, while all three models show substantial explanatory power for life satisfaction, the **Random Intercept with Random Slope for Education model** provides the highest overall fit, followed closely by the **Community model**. The inclusion of random slopes in both the education and community models improves the explanation of variability in life satisfaction across countries.

```{r, echo=FALSE, warning=FALSE, message = FALSE, fig.width=15, fig.height=8}

# Extract random effects
random_effects_mem <- ranef(fit_mem_ri)

# Plot random effects (use the random intercept and slope values)
dotplot(random_effects_mem)
```

```{r, echo=FALSE, warning=FALSE, message = FALSE, fig.width=15, fig.height=8}
# plotting the predicted and the original values on separate regression plots.

# Create a dataframe with the original data and predicted values
data_with_pred <- cbind(df_wb, pred = predict(fit_mem_ri))

ggplot(data_with_pred, aes(x = `Education.(0-10)`, y = `Life.satisfaction.(0-10)`, colour = Country)) +
  geom_point(alpha = 0.5) +
  geom_smooth(aes(y = pred), method = "lm", se = FALSE, linewidth = 1) + 
  facet_wrap(~ Country) + 
  theme_minimal() + 
  labs(title = "Regression Fit Lines for All Countries", x = "Education (0-10)", y = "Life Satisfaction (0-10)")
```

```{r, echo=FALSE, warning=FALSE, message = FALSE, fig.width=15, fig.height=8}
# plotting the predicted and the original values on separate regression plots.

# Create a dataframe with the original data and predicted values
data_with_pred_1 <- cbind(df_wb, pred = predict(fit_mem_ri_slope_edu))

ggplot(data_with_pred, aes(x = `Education.(0-10)`, y = `Life.satisfaction.(0-10)`, colour = Country)) +
  geom_point(alpha = 0.5) +
  geom_smooth(aes(y = pred), method = "lm", se = FALSE, linewidth = 1) + 
  facet_wrap(~ Country) + 
  theme_minimal() + 
  labs(title = "Regression Fit Lines for All Countries", x = "Education (0-10)", y = "Life Satisfaction (0-10)")
```

```{r, echo=FALSE, warning=FALSE, message = FALSE, fig.width=15, fig.height=8}
# plotting the predicted and the original values on separate regression plots.

# Create a dataframe with the original data and predicted values
data_with_pred_1 <- cbind(df_wb, pred = predict(fit_mem_ri_slope_comm))

ggplot(data_with_pred, aes(x = `Education.(0-10)`, y = `Life.satisfaction.(0-10)`, colour = Country)) +
  geom_point(alpha = 0.5) +
  geom_smooth(aes(y = pred), method = "lm", se = FALSE, linewidth = 1) + 
  facet_wrap(~ Country) + 
  theme_minimal() + 
  labs(title = "Regression Fit Lines for All Countries", x = "Education (0-10)", y = "Life Satisfaction (0-10)")
```

```{r, echo=FALSE, warning=FALSE, message = FALSE, fig.width=15, fig.height=8}
plot(fit_model_1$residuals)

plot(fitted(fit_model_1), residuals(fit_model_1))

hist(fit_model_1$residuals, breaks = 20)

qqnorm(fit_model_1$residuals)
qqline(fit_model_1$residuals)

# Residuals for Model 4 (Mixed-Effects Model)
residuals_mem <- residuals(fit_mem_ri)

# Plot residuals
plot(residuals_mem)

# Check for homoscedasticity (plot residuals vs fitted values)
plot(fitted(fit_mem_ri), residuals_mem)

# Check for normality of residuals (Histogram and Q-Q plot)
hist(residuals_mem, breaks = 20)

qqnorm(residuals_mem)
qqline(residuals_mem)

# Fitted values for Model 4 (Mixed-Effects Model)
fitted_values_mem <- fitted(fit_mem_ri)

# Plot residuals vs fitted values for homoscedasticity
plot(fitted_values_mem, residuals_mem)
```

\newpage

|   Dotplot of Random Effects from the Model with a Random Intercept and Random Slope:

```{r, echo=FALSE, warning=FALSE, message = FALSE, fig.width=15, fig.height=8}

# Extract random effects
random_effects_mem <- ranef(fit_mem_ri)

# Plot random effects (use the random intercept and slope values)
dotplot(random_effects_mem)
```


```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}

# Extract random effects
random_effects_mem_ed <- ranef(fit_mem_ri_slope_edu)

# Plot random effects (use the random intercept and slope values)
dotplot(random_effects_mem_ed)

```


```{r, echo=FALSE, warning=FALSE, fig.width=15, fig.height=8}
# Extract random effects
random_effects_mem_comm <- ranef(fit_mem_ri_slope_comm)

# Plot random effects (use the random intercept and slope values)
dotplot(random_effects_mem_comm)

```

\newpage

## Comparison of Models

```{r, echo=FALSE, warning=FALSE}

# Extract AIC and BIC for each model
aic_model_1 <- AIC(fit_model_1)
bic_model_1 <- BIC(fit_model_1)

aic_model_2 <- AIC(fit_model_2)
bic_model_2 <- BIC(fit_model_2)

aic_model_3 <- AIC(fit_model_3)
bic_model_3 <- BIC(fit_model_3)

aic_model_4 <- AIC(fit_mem_ri)
bic_model_4 <- BIC(fit_mem_ri)

aic_model_5 <- AIC(fit_mem_ri_slope_edu)
bic_model_5 <- BIC(fit_mem_ri_slope_edu)

aic_model_6 <- AIC(fit_mem_ri_slope_comm)
bic_model_6 <- BIC(fit_mem_ri_slope_comm)

# BIC Skipped, as we created complex models, and BIC always favors simple model


# Comparison Table
comparison_table <- data.frame(
  Model = c("Model 1", "Model 2", "Model 3", "Model 4 (Random Intercept)", "Model 5 (RI+S_Edu", "Model 6 (RI+S_Comm)"),
  AIC = c(aic_model_1, aic_model_2, aic_model_3, aic_model_4, aic_model_5, aic_model_6),
  BIC = c(bic_model_1, bic_model_2, bic_model_3, bic_model_4, bic_model_5, bic_model_6)
)

comparison_table

```

|   **Model 3** appears to provide the best fit in terms of AIC and BIC, suggesting it strikes the best balance between model complexity and accuracy, while the inclusion of random slopes (in Models 5 and 6) does not significantly improve the model's performance as indicated by the AIC/BIC comparison. From this comparison, we see that the Interaction effect no group model gives a better fit for the data, instead of the complex models. This can be due to the fact that we lack enough data points in each category, thus making the complex models work poorly.


# Conclusion

|   To conclude, the clustering analysis successfully grouped regions into distinct clusters, revealing significant disparities in well-being influenced by factors such as housing, community, and employment. The regression models provided robust insights into the relationships between these factors and **life satisfaction**. Taking into account that **Model 3** provides the best balance of accuracy and complexity, we can say that the main attributes the may influence **life satisfaction** are Income, Community, and Jobs (interacting with Civic Engagement). When it comes to interactions between these attributes, we can say that Education × Accessibility to Services and Safety × Housing are the most notable. However, future research could further refine these models by incorporating additional data and exploring temporal trends to better understand how life satisfaction evolves over time.

\newpage

# Appendix

## A: Summary of Clusters with Model Fitted

```{r, echo=FALSE, warning=FALSE}

# Summary of Clusters with Model Fitted by EM algorithm and a view counts in each cluster along with BIC values. 
summary(wb_optimal_cst, parameters = TRUE)

```

## B: Summary of Linear Regression using Pairwise Interaction effects

```{r, echo=FALSE, warning=FALSE}
summary(fit_model_2)
```